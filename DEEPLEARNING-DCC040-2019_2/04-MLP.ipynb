{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04.1 - Problemas.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ed03SC1Jm9Yy"
      },
      "source": [
        "## Aprendizado Profundo - UFMG\n",
        "\n",
        "## Problemas\n",
        "\n",
        "Como vimos acima, há muitos passos na criação e definição de uma nova rede neural.\n",
        "A grande parte desses ajustes dependem diretamente do problemas.\n",
        "\n",
        "Abaixo, listamos alguns problemas. Todos os problemas e datasets usados vem do [Center for Machine Learning and Intelligent Systems](http://archive.ics.uci.edu/ml/datasets.php).\n",
        "\n",
        "\n",
        "**Seu objetivo é determinar e implementar um modelo para cada problema.**\n",
        "\n",
        "Isso inclui definir uma arquitetura (por enquanto usando somente camadas [Densas](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Dense), porém podemos variar as ativações -- [Sigmoid](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.sigmoid), [Tanh](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.tanh), [ReLU](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.relu), [LeakyReLU, ELU, SeLU, PReLU, RReLU](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.LeakyReLU)), uma função de custo ( [L1](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L2Loss), [L2](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L1Loss),[ Huber](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HuberLoss), [*Cross-Entropy*](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss), [Hinge](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HingeLoss)), e um algoritmo de otimização ([SGD](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.SGD), [Momentum](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.SGD), [RMSProp](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.RMSProp), [Adam](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.Adam)).\n",
        "\n",
        "A leitura do dado assim como a função de treinamento já estão implementados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gp6CwWnFnTwb"
      },
      "source": [
        "Esse pequeno bloco de código abaixo é usado somente para instalar o MXNet para CUDA 10. Execute esse bloco somente uma vez e ignore possíveis erros levantados durante a instalação.\n",
        "\n",
        "**ATENÇÃO: a alteração deste bloco pode implicar em problemas na execução dos blocos restantes!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEzDaD4qXa75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "357b221f-3041-40b1-97fa-b80fb3afca54"
      },
      "source": [
        "#! pip install mxnet-cu100"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet-cu100 in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.16.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2019.6.16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ugeTFMXa79",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XW-VATPAldgt",
        "colab": {}
      },
      "source": [
        "# imports basicos\n",
        "\n",
        "from mxnet import autograd\n",
        "from mxnet import gluon\n",
        "from mxnet import init\n",
        "from mxnet import nd\n",
        "\n",
        "from mxnet.gluon import data as gdata\n",
        "from mxnet.gluon import loss as gloss\n",
        "from mxnet.gluon import nn\n",
        "from mxnet.gluon import utils as gutils\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import mxnet as mx\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xFbAGEeXa8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbV0V_yXXa8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "600aac4f-629b-4f8f-9270-69ec63413d83"
      },
      "source": [
        "# Tenta encontrar GPU\n",
        "def try_gpu():\n",
        "    try:\n",
        "        ctx = mx.gpu()\n",
        "        _ = nd.zeros((1,), ctx=ctx)\n",
        "    except mx.base.MXNetError:\n",
        "        ctx = mx.cpu()\n",
        "    return ctx\n",
        "\n",
        "ctx = try_gpu()\n",
        "ctx"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gpu(0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8oSVf8u1Oi1m",
        "colab": {}
      },
      "source": [
        "# funções básicas\n",
        "\n",
        "def load_array(features, labels, batch_size, is_train=True):\n",
        "    \"\"\"Construct a Gluon data loader\"\"\"\n",
        "    dataset = gluon.data.ArrayDataset(features, labels)\n",
        "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "def _get_batch(batch, ctx):\n",
        "    \"\"\"Return features and labels on ctx.\"\"\"\n",
        "    features, labels = batch\n",
        "    if labels.dtype != features.dtype:\n",
        "        labels = labels.astype(features.dtype)\n",
        "    return (gutils.split_and_load(features, ctx),\n",
        "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
        "\n",
        "# Função usada para calcular acurácia\n",
        "def evaluate_accuracy(data_iter, net, loss, ctx=[mx.cpu()]):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    if isinstance(ctx, mx.Context):\n",
        "        ctx = [ctx]\n",
        "    acc_sum, n, l = nd.array([0]), 0, 0\n",
        "    for batch in data_iter:\n",
        "        features, labels, _ = _get_batch(batch, ctx)\n",
        "        for X, y in zip(features, labels):\n",
        "            # X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
        "            y = y.astype('float32')\n",
        "            y_hat = net(X)\n",
        "            l += loss(y_hat, y).sum()\n",
        "            acc_sum += (y_hat.argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
        "            n += y.size\n",
        "        acc_sum.wait_to_read()\n",
        "    return acc_sum.asscalar() / n, l.asscalar() / n\n",
        "  \n",
        "    \n",
        "# Função usada no treinamento e validação da rede\n",
        "def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx,\n",
        "                   num_epochs, type='regression'):\n",
        "    print('training on', ctx)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
        "            with autograd.record():\n",
        "                y_hat = net(X)\n",
        "                l = loss(y_hat, y.astype('float32')).sum()\n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            y = y.astype('float32')\n",
        "            train_l_sum += l.asscalar()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "            n += y.size\n",
        "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss, ctx)\n",
        "        if type == 'regression':\n",
        "            print('epoch %d, train loss %.4f, test loss %.4f, time %.1f sec'\n",
        "                    % (epoch + 1, train_l_sum / n, test_loss, time.time() - start))\n",
        "        else:\n",
        "            print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, ' \\\n",
        "                  'test acc %.3f, time %.1f sec' % \\\n",
        "                  (epoch + 1, train_l_sum / n, train_acc_sum / n, test_loss, test_acc, time.time() - start))\n",
        "          \n",
        "        \n",
        "# funcao usada para teste\n",
        "def test(net, test_iter):\n",
        "    print('testing on', ctx)\n",
        "    first = True\n",
        "    for X in test_iter:\n",
        "        X = X.as_in_context(ctx)\n",
        "        y_hat = net(X)\n",
        "        if first is True:\n",
        "            pred_logits = y_hat\n",
        "            pred_labels = y_hat.argmax(axis=1)\n",
        "            first = False\n",
        "        else:\n",
        "            pred_logits = nd.concat(pred_logits, y_hat, dim=0)\n",
        "            pred_labels = nd.concat(pred_labels, y_hat.argmax(axis=1), dim=0)\n",
        "\n",
        "    return pred_logits.asnumpy(), pred_labels.asnumpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y0m-qic-0Wnl"
      },
      "source": [
        "## Problema 1\n",
        "\n",
        "Neste problema, você receberá 7 *features* extraídas de poços de petróleo ('BRCALI', 'BRDENS', 'BRDTP', 'BRGR', 'BRNEUT', 'BRRESC', 'BRRESP') e deve predizer o tipo de rocha."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U64ACnJoGsDv"
      },
      "source": [
        "### Treino e Validação\n",
        "\n",
        "Primeiro, vamos modelar uma rede neural e treiná-la.\n",
        "Usamos o dado de treino carregado no próximo bloco para convergir o modelo e o dado de validação para avaliar quão bom ele estão. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AUYOPZYH0Ztc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "38bf06a2-52b5-4db1-bebe-30e3a4dd0d39"
      },
      "source": [
        "#!wget https://www.dropbox.com/s/ujnqxh6l43tlbdi/poco_1.prn\n",
        "\n",
        "# Cria o dataset com um certo batch size a partir das features e labels\n",
        "def load_array(features, labels, batch_size, is_train=True):\n",
        "    dataset = gluon.data.ArrayDataset(features, labels)\n",
        "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "# Abre os dados\n",
        "X = np.loadtxt('poco_1.prn', skiprows=11, usecols=(1,2,3,4,5,6,7), dtype=np.float32)\n",
        "y = np.loadtxt('poco_1.prn', skiprows=11, usecols=8, dtype=np.str)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(list(set(y)))\n",
        "y_t = le.transform(y)\n",
        "\n",
        "# Pré processamento dos dados\n",
        "y_t_min, y_t_ptp = y_t.min(), y_t.ptp() # Parametros da normalização\n",
        "X_min, X_ptp = X.min(0), X.ptp(0) # Parametros da normalização\n",
        "\n",
        "y_t = (y_t - y_t_min) / y_t_ptp # Normalização de y\n",
        "X = (X - X_min) / X_ptp # Normalização de X\n",
        "\n",
        "# Divide os dados nos dados de treino e validação\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y_t, test_size=0.33)\n",
        "  \n",
        "# Definição dos dados de treino/validação\n",
        "batch_size = 10\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)\n",
        "\n",
        "# Criação da MLP\n",
        "net = gluon.nn.Sequential()\n",
        "net.add(gluon.nn.Dense(8, activation=\"relu\"))\n",
        "net.add(gluon.nn.Dense(1))\n",
        "  \n",
        "# Inicialização dos pesos\n",
        "net.collect_params().initialize(mx.init.Normal(sigma=0.1), ctx=ctx)\n",
        "\n",
        "# Otimizador - ADAM\n",
        "trainer = gluon.Trainer(net.collect_params(), 'adam')\n",
        "\n",
        "# Função de custo - L2\n",
        "loss = gluon.loss.L2Loss()\n",
        "\n",
        "# Treina o modelo\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx, 30)\n",
        "\n",
        "# Testa o modelo\n",
        "evaluate_accuracy(test_iter, net, loss, ctx)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on gpu(0)\n",
            "epoch 1, train loss 0.1118, test loss 0.0861, time 0.6 sec\n",
            "epoch 2, train loss 0.0707, test loss 0.0559, time 0.6 sec\n",
            "epoch 3, train loss 0.0545, test loss 0.0440, time 0.6 sec\n",
            "epoch 4, train loss 0.0477, test loss 0.0396, time 0.6 sec\n",
            "epoch 5, train loss 0.0438, test loss 0.0364, time 0.6 sec\n",
            "epoch 6, train loss 0.0411, test loss 0.0352, time 0.6 sec\n",
            "epoch 7, train loss 0.0394, test loss 0.0331, time 0.6 sec\n",
            "epoch 8, train loss 0.0378, test loss 0.0321, time 0.6 sec\n",
            "epoch 9, train loss 0.0368, test loss 0.0314, time 0.6 sec\n",
            "epoch 10, train loss 0.0362, test loss 0.0309, time 0.5 sec\n",
            "epoch 11, train loss 0.0354, test loss 0.0310, time 0.6 sec\n",
            "epoch 12, train loss 0.0350, test loss 0.0301, time 0.6 sec\n",
            "epoch 13, train loss 0.0347, test loss 0.0303, time 0.6 sec\n",
            "epoch 14, train loss 0.0345, test loss 0.0301, time 0.6 sec\n",
            "epoch 15, train loss 0.0342, test loss 0.0308, time 0.6 sec\n",
            "epoch 16, train loss 0.0342, test loss 0.0301, time 0.6 sec\n",
            "epoch 17, train loss 0.0342, test loss 0.0294, time 0.6 sec\n",
            "epoch 18, train loss 0.0337, test loss 0.0296, time 0.5 sec\n",
            "epoch 19, train loss 0.0339, test loss 0.0296, time 0.6 sec\n",
            "epoch 20, train loss 0.0336, test loss 0.0302, time 0.6 sec\n",
            "epoch 21, train loss 0.0329, test loss 0.0294, time 0.6 sec\n",
            "epoch 22, train loss 0.0331, test loss 0.0298, time 0.6 sec\n",
            "epoch 23, train loss 0.0333, test loss 0.0294, time 0.5 sec\n",
            "epoch 24, train loss 0.0335, test loss 0.0292, time 0.6 sec\n",
            "epoch 25, train loss 0.0331, test loss 0.0293, time 0.5 sec\n",
            "epoch 26, train loss 0.0329, test loss 0.0294, time 0.5 sec\n",
            "epoch 27, train loss 0.0330, test loss 0.0293, time 0.6 sec\n",
            "epoch 28, train loss 0.0330, test loss 0.0293, time 0.6 sec\n",
            "epoch 29, train loss 0.0327, test loss 0.0291, time 0.6 sec\n",
            "epoch 30, train loss 0.0330, test loss 0.0299, time 0.6 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6475849731663685, 0.029863906887648145)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pd8hG7HCDUib"
      },
      "source": [
        "## Problema 2\n",
        "\n",
        "Neste problema, você receberá várias *features* (como altura média, inclinação, etc) descrevendo uma região e o modelo deve predizer qual o tipo da região (floresta, montanha, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IZcIXGqBDznB",
        "outputId": "5fa020e2-8bdb-458d-bfef-d9a8f5a87330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "#!wget http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
        "#!gzip covtype.data.gz\n",
        "\n",
        "# Cria o dataset com um certo batch size a partir das features e labels\n",
        "def load_array(features, labels, batch_size, is_train=True):\n",
        "    dataset = gluon.data.ArrayDataset(features, labels)\n",
        "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "# Abre os dados\n",
        "data = np.genfromtxt('covtype.data', delimiter=',', dtype=np.float32)\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "# Pré processamento dos dados\n",
        "y_min, y_ptp = y.min(), y.ptp() # Parametros da normalização\n",
        "X_min, X_ptp = X.min(0), X.ptp(0) # Parametros da normalização\n",
        "\n",
        "y = (y - y_min) / y_ptp # Normalização de y\n",
        "X = (X - X_min) / X_ptp # Normalização de X\n",
        "\n",
        "# Divide os dados nos dados de treino e validação\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "  \n",
        "# Definição dos dados de treino/validação\n",
        "batch_size = 128\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)\n",
        "\n",
        "# Criação da MLP\n",
        "net = gluon.nn.Sequential()\n",
        "net.add(gluon.nn.Dense(32, activation=\"relu\"))\n",
        "net.add(gluon.nn.Dense(8, activation=\"relu\"))\n",
        "net.add(gluon.nn.Dense(1))\n",
        " \n",
        "# Inicialização dos pesos\n",
        "net.collect_params().initialize(mx.init.Normal(sigma=0.1), ctx=ctx)\n",
        "\n",
        "# Otimizador - ADAM\n",
        "trainer = gluon.Trainer(net.collect_params(), 'adam')\n",
        "\n",
        "# Função de custo - L2\n",
        "loss = gluon.loss.L2Loss()\n",
        "\n",
        "# Treina o modelo\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx, 30)\n",
        "\n",
        "# Testa o modelo\n",
        "evaluate_accuracy(test_iter, net, loss, ctx)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on gpu(0)\n",
            "epoch 1, train loss 0.0164, test loss 0.0146, time 18.6 sec\n",
            "epoch 2, train loss 0.0140, test loss 0.0135, time 18.4 sec\n",
            "epoch 3, train loss 0.0132, test loss 0.0133, time 19.2 sec\n",
            "epoch 4, train loss 0.0127, test loss 0.0124, time 18.5 sec\n",
            "epoch 5, train loss 0.0123, test loss 0.0123, time 18.7 sec\n",
            "epoch 6, train loss 0.0121, test loss 0.0121, time 18.5 sec\n",
            "epoch 7, train loss 0.0119, test loss 0.0126, time 18.6 sec\n",
            "epoch 8, train loss 0.0117, test loss 0.0120, time 19.1 sec\n",
            "epoch 9, train loss 0.0116, test loss 0.0115, time 18.6 sec\n",
            "epoch 10, train loss 0.0115, test loss 0.0114, time 18.6 sec\n",
            "epoch 11, train loss 0.0114, test loss 0.0118, time 18.4 sec\n",
            "epoch 12, train loss 0.0113, test loss 0.0113, time 18.6 sec\n",
            "epoch 13, train loss 0.0112, test loss 0.0110, time 18.8 sec\n",
            "epoch 14, train loss 0.0111, test loss 0.0111, time 18.6 sec\n",
            "epoch 15, train loss 0.0111, test loss 0.0113, time 18.6 sec\n",
            "epoch 16, train loss 0.0110, test loss 0.0110, time 18.7 sec\n",
            "epoch 17, train loss 0.0109, test loss 0.0108, time 18.5 sec\n",
            "epoch 18, train loss 0.0109, test loss 0.0115, time 18.4 sec\n",
            "epoch 19, train loss 0.0108, test loss 0.0110, time 18.5 sec\n",
            "epoch 20, train loss 0.0107, test loss 0.0109, time 18.5 sec\n",
            "epoch 21, train loss 0.0107, test loss 0.0106, time 18.4 sec\n",
            "epoch 22, train loss 0.0106, test loss 0.0106, time 18.5 sec\n",
            "epoch 23, train loss 0.0106, test loss 0.0109, time 18.4 sec\n",
            "epoch 24, train loss 0.0105, test loss 0.0109, time 18.4 sec\n",
            "epoch 25, train loss 0.0105, test loss 0.0112, time 18.9 sec\n",
            "epoch 26, train loss 0.0104, test loss 0.0103, time 18.5 sec\n",
            "epoch 27, train loss 0.0104, test loss 0.0104, time 18.3 sec\n",
            "epoch 28, train loss 0.0103, test loss 0.0103, time 18.2 sec\n",
            "epoch 29, train loss 0.0103, test loss 0.0103, time 18.8 sec\n",
            "epoch 30, train loss 0.0103, test loss 0.0106, time 18.4 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.36497439160503614, 0.01064777200179513)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IDaRVNq1aMpm"
      },
      "source": [
        "## Problema 3\n",
        "\n",
        "Neste problema, você receberá 90 *features* extraídas de diversas músicas (datadas de 1922 até 2011) e deve predizer o ano de cada música."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWdBT3zhW_Y5",
        "outputId": "ee26a90e-39da-458f-cf33-cac650db3c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "#!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
        "#!unzip YearPredictionMSD.txt.zip\n",
        "\n",
        "# Cria o dataset com um certo batch size a partir das features e labels\n",
        "def load_array(features, labels, batch_size, is_train=True):\n",
        "    dataset = gluon.data.ArrayDataset(features, labels)\n",
        "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "# Abre os dados\n",
        "data = np.genfromtxt('YearPredictionMSD.txt', delimiter=',', dtype=np.float32)\n",
        "X, y = data[:, 1:], data[:, 0]\n",
        "\n",
        "# Pré processamento dos dados\n",
        "y_min, y_ptp = y.min(), y.ptp() # Parametros da normalização\n",
        "X_min, X_ptp = X.min(0), X.ptp(0) # Parametros da normalização\n",
        "\n",
        "y = (y - y_min) / y_ptp # Normalização de y\n",
        "X = (X - X_min) / X_ptp # Normalização de X\n",
        "\n",
        "# Divide os dados nos dados de treino e validação\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "  \n",
        "# Definição dos dados de treino/validação\n",
        "batch_size = 128\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)\n",
        "\n",
        "# Criação da MLP\n",
        "net = gluon.nn.Sequential()\n",
        "net.add(gluon.nn.Dense(8, activation=\"relu\"))\n",
        "net.add(gluon.nn.Dense(1))\n",
        "  \n",
        "# Inicialização dos pesos\n",
        "net.collect_params().initialize(mx.init.Normal(sigma=0.1), ctx=ctx)\n",
        "\n",
        "# Otimizador - ADAM\n",
        "trainer = gluon.Trainer(net.collect_params(), 'adam')\n",
        "\n",
        "# Função de custo - L2\n",
        "loss = gluon.loss.L2Loss()\n",
        "\n",
        "# Treina o modelo\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx, 30)\n",
        "\n",
        "# Testa o modelo\n",
        "evaluate_accuracy(test_iter, net, loss, ctx)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on gpu(0)\n",
            "epoch 1, train loss 0.0093, test loss 0.0071, time 14.5 sec\n",
            "epoch 2, train loss 0.0066, test loss 0.0062, time 14.4 sec\n",
            "epoch 3, train loss 0.0061, test loss 0.0060, time 14.4 sec\n",
            "epoch 4, train loss 0.0060, test loss 0.0059, time 14.8 sec\n",
            "epoch 5, train loss 0.0060, test loss 0.0059, time 14.6 sec\n",
            "epoch 6, train loss 0.0059, test loss 0.0059, time 14.4 sec\n",
            "epoch 7, train loss 0.0059, test loss 0.0059, time 14.7 sec\n",
            "epoch 8, train loss 0.0059, test loss 0.0058, time 14.4 sec\n",
            "epoch 9, train loss 0.0059, test loss 0.0058, time 14.4 sec\n",
            "epoch 10, train loss 0.0059, test loss 0.0058, time 14.5 sec\n",
            "epoch 11, train loss 0.0058, test loss 0.0061, time 14.4 sec\n",
            "epoch 12, train loss 0.0057, test loss 0.0057, time 14.3 sec\n",
            "epoch 13, train loss 0.0057, test loss 0.0056, time 14.4 sec\n",
            "epoch 14, train loss 0.0057, test loss 0.0058, time 14.3 sec\n",
            "epoch 15, train loss 0.0057, test loss 0.0056, time 14.2 sec\n",
            "epoch 16, train loss 0.0056, test loss 0.0059, time 14.5 sec\n",
            "epoch 17, train loss 0.0056, test loss 0.0057, time 14.4 sec\n",
            "epoch 18, train loss 0.0056, test loss 0.0059, time 14.3 sec\n",
            "epoch 19, train loss 0.0056, test loss 0.0056, time 14.4 sec\n",
            "epoch 20, train loss 0.0056, test loss 0.0056, time 14.4 sec\n",
            "epoch 21, train loss 0.0056, test loss 0.0056, time 14.3 sec\n",
            "epoch 22, train loss 0.0056, test loss 0.0057, time 14.4 sec\n",
            "epoch 23, train loss 0.0056, test loss 0.0056, time 14.3 sec\n",
            "epoch 24, train loss 0.0056, test loss 0.0056, time 14.4 sec\n",
            "epoch 25, train loss 0.0056, test loss 0.0056, time 14.4 sec\n",
            "epoch 26, train loss 0.0056, test loss 0.0057, time 15.0 sec\n",
            "epoch 27, train loss 0.0056, test loss 0.0057, time 14.4 sec\n",
            "epoch 28, train loss 0.0056, test loss 0.0057, time 14.9 sec\n",
            "epoch 29, train loss 0.0056, test loss 0.0057, time 14.4 sec\n",
            "epoch 30, train loss 0.0056, test loss 0.0057, time 14.3 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.1760278483394486e-05, 0.0056996874441846155)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    }
  ]
}