{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "02-Linear e Logistica.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e7OS4zk9520",
        "colab_type": "text"
      },
      "source": [
        "# Aprendizado Profundo - UFMG\n",
        "\n",
        "## Preâmbulo\n",
        "\n",
        "O código abaixo consiste dos imports comuns. Além do mais, configuramos as imagens para ficar de um tamanho aceitável e criamos algumas funções auxiliares. No geral, você pode ignorar a próxima célula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwbBZz2y9522",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install mxnet-cu100==1.4.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPAyiOZV9528",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf8\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import mxnet as mx\n",
        "import mxnet.ndarray as nd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "plt.rcParams['figure.figsize']  = (18, 10)\n",
        "plt.rcParams['axes.labelsize']  = 20\n",
        "plt.rcParams['axes.titlesize']  = 20\n",
        "plt.rcParams['legend.fontsize'] = 20\n",
        "plt.rcParams['xtick.labelsize'] = 20\n",
        "plt.rcParams['ytick.labelsize'] = 20\n",
        "plt.rcParams['lines.linewidth'] = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN2v2Mzx953E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.ion()\n",
        "\n",
        "plt.style.use('seaborn-colorblind')\n",
        "plt.rcParams['figure.figsize']  = (12, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "synjrjfQ953I",
        "colab_type": "text"
      },
      "source": [
        "Para testar o resultado dos seus algoritmos vamos usar o módulo testing do numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7HPft73953J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.testing import assert_equal\n",
        "from numpy.testing import assert_almost_equal\n",
        "from numpy.testing import assert_array_almost_equal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0IUKjwo953N",
        "colab_type": "text"
      },
      "source": [
        "## Aula 02 - Regressão Linear e Logística from Scratch\n",
        "\n",
        "Um fator importante de salientar é que embora nosso curso está fazendo uso de mxnet nas aulas iniciais, vamos migrar para outros frameworks estilo PyTorch ao longo das aulas. No fim, todos são bem parecidos e aplicam as mesmas ideias de diferenciação automática. Para brincar um pouco mais com essa diferenciação, nesta aula vamos implementar a regressão linear e logística do zero. Vamos fazer duas versões de cada:\n",
        "\n",
        "1. Derivando na mão, não é complicado.\n",
        "2. Derivando com mxnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9GdDALV953Q",
        "colab_type": "text"
      },
      "source": [
        "## Conjunto de Problemas 1: Mais Derivadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMvfj46e953S",
        "colab_type": "text"
      },
      "source": [
        "Antes de entrar na regressão, vamos brincar um pouco de derivadas dentro de funções. Dado dois números `x` and `y`, implemente a função `log_exp` que retorna:\n",
        "\n",
        "$$-\\log\\left(\\frac{e^x}{e^x+e^y}\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3n2nG5A953U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_exp(x, y):\n",
        "    # implemente\n",
        "    return -nd.log(np.e**x / (np.e**x + np.e**y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71L4gKLP953Z",
        "colab_type": "text"
      },
      "source": [
        "1. Abaixo vamos testar o seu código com algumas entradas simples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHEhz4dI953a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = nd.array([2]), nd.array([3])\n",
        "z = log_exp(x, y)\n",
        "z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pb8AP63953e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Teste. Não apague\n",
        "assert_equal(1.31326175, z.asnumpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnJi9Sa7953k",
        "colab_type": "text"
      },
      "source": [
        "2. Agora implementa uma função para computar $\\partial z/\\partial x$ e $\\partial z/\\partial y$ usando `autograd`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiaSipKW953n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# O argumento funcao_forward é uma função python. Será a sua log_exp.\n",
        "# A ideia aqui é deixar claro a ideia de forward e backward propagation, depois\n",
        "# de avaliar a função chamamos backward e temos as derivadas.\n",
        "def grad(funcao_forward, x, y):\n",
        "    x.attach_grad()\n",
        "    y.attach_grad()\n",
        "    \n",
        "    with mx.autograd.record():\n",
        "      z = funcao_forward(x, y)\n",
        "      \n",
        "    z.backward()\n",
        "    \n",
        "    return x.grad, y.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl6CNa-t953t",
        "colab_type": "text"
      },
      "source": [
        "Testando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j4s-2rc953v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = nd.array([2]), nd.array([3])\n",
        "dx, dy = grad(log_exp, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBEUySV9953z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert_equal(-0.7310586, dx.asnumpy())\n",
        "assert_equal(0.7310586, dy.asnumpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6udXgGub9534",
        "colab_type": "text"
      },
      "source": [
        "4. Agora teste com números maiores, algum problema?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WtguK_h9536",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = nd.array([50]), nd.array([100])\n",
        "grad(log_exp, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EndiLSWY953_",
        "colab_type": "text"
      },
      "source": [
        "5. Pense um pouco sobre o motivo do erro acima. Usando as propriedade de logaritmos, é possível fazer uma função mais estável? Implemente a mesma abaixo. O problema aqui é que o exponencial \"explode\" quando x ou y são muito grandes. Use o [link](http://www.wolframalpha.com/input/?i=log[e%5Ex+%2F+[e%5Ex+%2B+e%5Ey]]) para lhe ajudar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17434hIt954A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stable_log_exp(x, y):\n",
        "  return nd.log(np.e**(y - x) + 1)\n",
        "  # return np.log(np.e**x + np.e**y) - x\n",
        "\n",
        "dx, dy = grad(stable_log_exp, x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMTehnNT954E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stable_log_exp(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxWp-JuT954H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Teste. Não apague\n",
        "assert_equal(-1, dx.asnumpy())\n",
        "assert_equal(1, dy.asnumpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc2KrJfa954N",
        "colab_type": "text"
      },
      "source": [
        "O exemplo acima mostra um pouco de problemas de estabilidade númerica. às vezes é melhor usar versões alternativas de funções. Isto vai ocorrer quando você ver vários nans na sua frente :-) Claro, estamos assumindo que existe uma outra função equivalente que é mais estável para o computador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snvW54DZ954O",
        "colab_type": "text"
      },
      "source": [
        "## Conjunto de Problemas 2: Regressão Linear\n",
        "\n",
        "Agora, vamos explorar uma regressão linear. Embora não vamos fazer uso da logexp acima, a ideia de derivar parcialmente dentro de funções pode nos ajudar. Lembrando da regressão linear, inicialmente temos um conjunto observações representadas como tuplas $(\\mathbf{x}_i, y_i)$. Aqui, $\\mathbf{x}_i$ é um vetor de atributos. Vamo forçar $\\mathbf{x}_{i0} = 1$, capturando assim o intercepto. Além do mais, $\\mathbf{x}_{ij}$ quando $j\\neq 0$, são os outros atributos de entrada. $y_i$ uma valor real representando uma resposta. Nossa regressão visa capturar:\n",
        "\n",
        "$$y_i = 1 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\cdots + \\theta_k x_{if}$$\n",
        "\n",
        "Lembrando da regressão linear multivariada, podemos representar as equações como uma multiplicação de uma matriz com um vetor\n",
        "\n",
        "![](./figs/linear.png)\n",
        "\n",
        "7. Crie uma função de previsao. A mesma recebe uma matrix $\\mathbf{X}$ e um vetor de parâmetros $\\theta$. Sua função deve retornar um vetor de previsões para cada linha de $\\mathbf{X}$. Não use nenhum laço!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayBFUlpi954Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def previsao(X, theta):\n",
        "    return nd.dot(X, theta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT9BrMT5954V",
        "colab_type": "text"
      },
      "source": [
        "**Erros quadrados**. Para aprender os parâmetros ótimos da regressão linear, precisamos fazer uso de um modelo de erros quadrados. Em particular nosso objetico é aprender os parâmetros que minimizam a função:\n",
        "\n",
        "$$L(\\mathbf{\\theta}) = n^{-1} \\sum_i ({\\hat{y}_i - y_i})^2$$\n",
        "\n",
        "Onde $\\hat{y}_i$ é uma previsão (vêm da sua função python acima). $y_i$ é o valor real dos dados.\n",
        "\n",
        "8. Implemente uma função para a média dos os erros quadrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoGRuoFY954X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def media_erros_quadrados(X, theta, y):\n",
        "    return ((previsao(X, theta) - y)**2).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yv7BbjT954c",
        "colab_type": "text"
      },
      "source": [
        "9. Agora, crie **DUAS** funções que derivam o erro acima. A primeira deve usar o autograd de mxnet. Lembre-se que temos um vetor de parâmetros $\\theta$. Por sorte, você pode fazer derivadas de tais vetores também."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crf_Gto7954g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def derivada_mxnet(X, theta, y):\n",
        "    theta.attach_grad()\n",
        "    \n",
        "    with mx.autograd.record():\n",
        "      z = media_erros_quadrados(X, theta, y)\n",
        "      \n",
        "    z.backward()\n",
        "    \n",
        "    return theta.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeYKA4cU954r",
        "colab_type": "text"
      },
      "source": [
        "10. A segunda versão não usa mxnet. Implemente as derivadas do zero. Nos [slides](https://docs.google.com/presentation/d/1bz3G3fEohNtvERKSDtThfGPYOjF83Fzy2yHPdCAlFZw/edit#slide=id.g584f66d2ae_3_74) do link explicamos como fazer tal operação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5EKlGh1954t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def derivada_navera(X, theta, y):\n",
        "  return -2 * ((y - nd.dot(X, theta)) * X.T).mean(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQx7OB7l954x",
        "colab_type": "text"
      },
      "source": [
        "11. Por fim, otimize sua função usando o algoritmo de gradiente descendente abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inZDQ3MG954z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gd(d_fun, loss_fun, X, y, lambda_=0.01, tol=0.00001, max_iter=10000):\n",
        "    '''\n",
        "    Executa Gradiente Descendente. Aqui:\n",
        "    \n",
        "    Parâmetros\n",
        "    ----------\n",
        "    d_fun : é uma função de derivadas\n",
        "    loss_fun : é uma função de perda\n",
        "    X : é um vetor de fatores explanatórios.\n",
        "        Copie seu código de intercepto da primeira aula.\n",
        "        para adicionar o intercepto em X.\n",
        "    y : é a resposta\n",
        "    lambda : é a taxa de aprendizad\n",
        "    tol : é a tolerância, define quando o algoritmo vai parar.\n",
        "    max_ter : é a segunda forma de parada, mesmo sem convergir\n",
        "              paramos depois de max_iter iterações.\n",
        "    '''\n",
        "    theta = nd.random.normal(shape=X.shape[1])\n",
        "    print('Iter {}; theta = '.format(0), theta)\n",
        "    old_err_sq = np.inf\n",
        "    i = 0\n",
        "    while True:\n",
        "        # Computar as derivadas\n",
        "        grad = d_fun(X, theta, y)\n",
        "        # Atualizar\n",
        "        theta_novo = theta - lambda_ * grad\n",
        "        \n",
        "        # Parar quando o erro convergir\n",
        "        err_sq = loss_fun(X, theta, y)\n",
        "        if nd.abs(old_err_sq - err_sq) <= tol:\n",
        "            break\n",
        "        \n",
        "        # Atualizar parâmetros e erro\n",
        "        theta = theta_novo\n",
        "        old_err_sq = err_sq\n",
        "        \n",
        "        # Informação de debug\n",
        "        print('Iter {}; theta = '.format(i+1), theta)\n",
        "        i += 1\n",
        "        if i == max_iter:\n",
        "            break\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9SUCy969545",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Para testes, não apague!!!\n",
        "X = nd.zeros(shape=(1000, 2))\n",
        "X[:, 0] = 1\n",
        "X[:, 1] = nd.random.normal(shape=1000)\n",
        "\n",
        "theta_0_real = 7\n",
        "theta_1_real = 9\n",
        "y = theta_0_real + theta_1_real * X[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8Przynu9549",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sua função deve retornar algo perto de 7 e 9\n",
        "theta = gd(derivada_mxnet, media_erros_quadrados, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAAVbVJG955A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3gjV2BF955D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sua função deve retornar algo perto de 7 e 9\n",
        "theta = gd(derivada_navera, media_erros_quadrados, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuyeyXBT955I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TRpi7iF955M",
        "colab_type": "text"
      },
      "source": [
        "12. Altere a função de Gradiente Descendente para funcionar com minibatches. Em outras palavras, não compute o erro usando todos os dados de X. Use um `slice` de tamanho do minibatch. Uma ideia é seguir o pseudocódigo abaixo.\n",
        "\n",
        "```python\n",
        "index = np.arange(len(X))\n",
        "while True:\n",
        "    minib = np.random.choice(index, minibatchsize) # aqui estou usando numpy para selection minibatch elementos\n",
        "    X_batch = X[minib]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Jp_jD6955N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exemplo abaixo\n",
        "index = np.arange(len(X))\n",
        "mb = np.random.choice(index, 50)\n",
        "print(mb)\n",
        "X[mb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZP6s4kA955S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minibatch_gd(d_fun, loss_fun, X, y, lambda_=0.01, tol=0.00001,\n",
        "                 max_iter=10000, batch_size=10):\n",
        "    '''\n",
        "    Executa Gradiente Descendente. Aqui:\n",
        "    \n",
        "    Parâmetros\n",
        "    ----------\n",
        "    d_fun : é uma função de derivadas\n",
        "    loss_fun : é uma função de perda\n",
        "    X : é um vetor de fatores explanatórios.\n",
        "        Copie seu código de intercepto da primeira aula.\n",
        "        para adicionar o intercepto em X.\n",
        "    y : é a resposta\n",
        "    lambda : é a taxa de aprendizad\n",
        "    tol : é a tolerância, define quando o algoritmo vai parar.\n",
        "    max_ter : é a segunda forma de parada, mesmo sem convergir\n",
        "              paramos depois de max_iter iterações.\n",
        "    batch_size : tamanho do batch\n",
        "    '''  \n",
        "    \n",
        "    ids = np.arange(X.shape[0])\n",
        "    batch_ids = np.random.choice(ids, batch_size)\n",
        "    batch_X = nd.take(X, nd.array(batch_ids))\n",
        "    batch_y = nd.take(y, nd.array(batch_ids))\n",
        "    \n",
        "    theta = nd.random.normal(shape=batch_X.shape[1])\n",
        "    print('Iter {}; theta = '.format(0), theta)\n",
        "    old_err_sq = np.inf\n",
        "    i = 0\n",
        "    while True:\n",
        "        # Computar as derivadas\n",
        "        grad = d_fun(batch_X, theta, batch_y)\n",
        "        # Atualizar\n",
        "        theta_novo = theta - lambda_ * grad\n",
        "        \n",
        "        # Parar quando o erro convergir\n",
        "        err_sq = loss_fun(batch_X, theta, batch_y)\n",
        "        if nd.abs(old_err_sq - err_sq) <= tol:\n",
        "            break\n",
        "        \n",
        "        # Atualizar parâmetros e erro\n",
        "        theta = theta_novo\n",
        "        old_err_sq = err_sq\n",
        "        \n",
        "        # Informação de debug\n",
        "        print('Iter {}; theta = '.format(i+1), theta)\n",
        "        i += 1\n",
        "        if i == max_iter:\n",
        "            break\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amaDmOBo955Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sua função deve retornar algo perto de 7 e 9\n",
        "minibatch_gd(derivada_mxnet, media_erros_quadrados, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jAxbcOg955g",
        "colab_type": "text"
      },
      "source": [
        "## Conjunto de Problemas 3: Logistic from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4ASYeQk955i",
        "colab_type": "text"
      },
      "source": [
        "12. Repita o mesmo processo para a Logística. Lembrando que a mesma tem a seguinte forma:\n",
        "\n",
        "$$f(x_i) = \\frac{1}{1 + e^{-(1 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\cdots \\theta_k x_{ik})}}$$\n",
        "\n",
        "Implemente a função logística."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn5ucAMG955j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistic(X, theta):\n",
        "    return 1.0 / (1.0 + np.e**-(nd.dot(X, theta)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqHkCKTC955n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testes, não apague!\n",
        "X_teste = nd.random.normal(shape=(1000, 20000))\n",
        "theta = nd.random.normal(shape=(20000))\n",
        "y_hat_teste = logistic(X_teste, theta)\n",
        "assert_equal(True, (y_hat_teste >= 0).asnumpy().all())\n",
        "assert_equal(True, (y_hat_teste <= 1).asnumpy().all())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS3NXAq3955p",
        "colab_type": "text"
      },
      "source": [
        "Usando a logística acima implemente uma função logistica_prever que retorna 0 ou 1. Use o limar dado na função. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9RqnD30955r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistica_prever(X, theta, limiar=0.5):\n",
        "  return nd.array([1 if logistic(X[i], theta) > limiar else 0 for i in range(X.shape[0])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10E6NvKY955y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testes, não apague!\n",
        "X_teste = nd.random.normal(shape=(1000, 20000))\n",
        "theta = nd.random.normal(shape=(20000))\n",
        "y_hat_teste = logistica_prever(X_teste, theta)\n",
        "for yi in y_hat_teste.asnumpy():\n",
        "    assert(yi in {0, 1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ytWHb3T9551",
        "colab_type": "text"
      },
      "source": [
        "Agora, implemente uma função de entropia cruzada da logística. A mesma, é proporcional ao inverso da verossimilhança. Para entender a derivação entre as duas faça uso dos [Slides](https://docs.google.com/presentation/d/1yGPETPe8o7PPOP6_CF38LHr3vpxgTEnF5LjP-1pkGIc/edit?usp=sharing). \n",
        "\n",
        "Sendo:\n",
        "\n",
        "$$ll(x_i,y_i~|~\\theta) = y_i \\log f(x_i\\theta) + (1-y_i) \\log (1-f(x_i\\theta))$$\n",
        "\n",
        "A verossimilhança para uma observação. A entropia cruzada é a media da negação do termo para todos os exemplos:\n",
        "\n",
        "$$L(\\theta) = -n^{-1}\\sum_i \\big((1-y_i)\\log (1-f_{\\theta}(x_i)) + y_i\\log (f_{\\theta}(x_i))\\big)$$\n",
        "\n",
        "`dica: use nd.clip(logistic(X, theta), 0.001, 0.999)`. A função remove os 0 e 1s da logistic, evitando assim o valor log(0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqmdkkGz9553",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_mean(X, theta, y):\n",
        "    l = nd.clip(logistic(X, theta), 0.001, 0.999)\n",
        "    return -((y > 0.5) * nd.log(l) + (1 - (y > 0.5)) * nd.log(1 - l)).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPHkPJ2T9558",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testes, não apague!\n",
        "from sklearn import datasets\n",
        "state = np.random.seed(20190187)\n",
        "\n",
        "X, y = datasets.make_blobs(n_samples=200, n_features=2, centers=2)\n",
        "X = nd.array(X)\n",
        "y = nd.array(y)\n",
        "\n",
        "for _ in range(100):\n",
        "    theta = nd.random.normal(shape=(2, ))\n",
        "    assert(cross_entropy_mean(X, theta, y) >= 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksCJ7ncA956C",
        "colab_type": "text"
      },
      "source": [
        "Agora implemente a derivada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTboPQHi956D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def derivada_mxnet_logit(X, theta, y):\n",
        "    theta.attach_grad()\n",
        "    \n",
        "    with mx.autograd.record():\n",
        "      z = cross_entropy_mean(X, theta, y)\n",
        "      \n",
        "    z.backward()\n",
        "    \n",
        "    return theta.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ6o0cnd956G",
        "colab_type": "text"
      },
      "source": [
        "A partir daqui basta executar código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0UzlaAX956H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(X.asnumpy()[:, 0], X.asnumpy()[:, 1], s=80, edgecolors='k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GwBxZWe956K",
        "colab_type": "text"
      },
      "source": [
        "13. Assim como foi feito na primeira aula, abaixo testamos o seu código com um toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7tgQ06s956L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use essa função antes de executar o GD\n",
        "def add_intercept(X):\n",
        "    Xn = nd.zeros(shape=(X.shape[0], X.shape[1] + 1))\n",
        "    Xn[:, 0]  = 1\n",
        "    Xn[:, 1:] = X\n",
        "    return Xn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3SSyK7g956O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xn = add_intercept(X)\n",
        "theta = minibatch_gd(derivada_mxnet_logit, cross_entropy_mean, Xn, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka0zPsN5956Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_p = logistica_prever(Xn, theta)\n",
        "print((y == y_p).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RG1zRds956d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}