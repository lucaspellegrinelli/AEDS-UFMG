{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S05A03_2 - Convolutional GANs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do0o-c0P9wcM",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ATPZX-9qXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuJgxyO-9z-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 25,      # Number of epochs.\n",
        "    'lr': 5e-4,           # Learning rate.\n",
        "    'weight_decay': 5e-5, # L2 penalty.\n",
        "    'num_workers': 8,     # Number of workers on data loader.\n",
        "    'batch_size': 500,    # Mini-batch size.\n",
        "    'print_freq': 1,      # Printing frequency.\n",
        "    'z_dim': 100,         # Dimension of z input vector.\n",
        "    'num_samples': 4,     # Number of samples to be generated in evaluation.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZA6okfQ98TP",
        "colab_type": "text"
      },
      "source": [
        "# Carregando o  MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQAwioI99NJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Transformations over the dataset.\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.MNIST(root,\n",
        "                           train=True,\n",
        "                           download=True,\n",
        "                           transform=data_transforms)\n",
        "test_set = datasets.MNIST(root,\n",
        "                          train=False,\n",
        "                          download=False,\n",
        "                          transform=data_transforms)\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHV0enX9-Ars",
        "colab_type": "text"
      },
      "source": [
        "# Treinamento Adversarial Convolucional\n",
        "\n",
        "Como desde o começo GANs foram pensadas para imagens primariamente, era de se esperar que convoluções fossem inseridas em algum momento. O artigo original das [GANs](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), inclusive, contém testes entre arquiteturas parcialmente convolucionais e compostas apenas por camadas FC:\n",
        "\n",
        "*   Resultados Fully Connected no CIFAR;\n",
        "![FC GAN](https://www.dropbox.com/s/dbem2z7jjzodoyb/gan_fc_goodfellow.png?dl=1)\n",
        "*   Resultados Convolutionais no CIFAR.\n",
        "![Convolutional GAN](https://www.dropbox.com/s/z9ihvqb4a53eqvb/gan_conv_goodfellow.png?dl=1)\n",
        "\n",
        "A arquitetura de $G$ lembra o Decoder de um VAE com camadas que partem de um vetor aleatório $z$ e geram uma amostra final sintética $x$. Já a arquitetura de $D$ lembra uma CNN tradicional para classificação de imagens, como a AlexNet, VGG, ResNet ou DenseNet que já vimos previamente no curso:\n",
        "\n",
        "*   Arquitetura de uma Generativa $G$;\n",
        "![Generator Architecture](https://www.dropbox.com/s/yf4d4sb1xcv8bma/GANs_Architecture_G.png?dl=1)\n",
        "\n",
        "*   Arquitetura de uma Discriminativa $D$.\n",
        "![Discriminator Architecture](https://www.dropbox.com/s/72s95njsuuag5m6/GANs_Architecture_D.png?dl=1)\n",
        "\n",
        "Por muito tempo, porém, foi proibitivo criar GAN convolucionais com muitas camadas, pois haviam problemas sérios de convergência e instabilidade no treinamento. O artigo das [Deep Convolutional GANs](https://arxiv.org/pdf/1511.06434.pdf) mitigou a maior parte desses problemas, propondo uma arquitetura padrão, e hoje em dia é possível treinar uma Convolutional GAN com diversas camadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc44m3gCGYmx",
        "colab_type": "text"
      },
      "source": [
        "# Atividade Prática: Implementando uma GAN\n",
        "\n",
        "Nessa atividade implementaremos os principais elementos de uma GAN tradicional de acordo com os passos a seguir:\n",
        "\n",
        "1.   Implemente a rede generativa $G$ que vai otimizar a distribuição $p(z | x)$. $G$ é composta de dois blocos sequenciais: a) o bloco *self.fc* que codifica $z$ para uma dimensionalidade maior que case com a entrada das convoluções transpostas; e b) o bloco *self.deconv* que conta com convoluções transpostas que realizam o upsampling aprendido nas imagens. Devem ser colocados dois blocos lineares dentro de *self.fc* (linear, batchnorm1d e relu) e dois blocos de convolução transposta (convtranspose2d, batchnorm2d, relu, convtranspose2d, sigmoid) em *self.deconv*;\n",
        "2.   Implemente a rede discriminativa $D$ que vai otimizar a probabilidade das imagens de entrada serem reais ou falsas. Essa rede será basicamnete uma CNN com arquitetura quase simétrica a $G$, composta por dois blocos sequenciais: a) *self.conv* composto por dois blocos convolucionais (conv2d, batchnorm2d, leakyrely, conv2d, sigmoid); e b) dois blocos lineares como em $G$, mas com uma ativação do último bloco sendo linear sendo uma sigmoide e sem batchnorm1d;\n",
        "3.   Defina dessa vez **dois** otimizadores, um para os parâmetros de $G$ e um para os parâmetros de $D$;\n",
        "4.   Defina [schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) de learning rate do tipo [StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) para diminuir a Learning Rate a cada 5 epochs. Um scheduler deve atender a cada otimizador;\n",
        "5.   Definir o criterion da loss composta. Apesar da função de loss ser oposta entre $G$ e $D$, só precisamos definir o criterion uma vez. Em GANs tradicionais, usamos a BCE;\n",
        "6. Complemente a função *train()*.\n",
        "\n",
        "PS. 1: em $G$ a saída da *self.fc* deve ter dimensões $(B, 128 \\times 7 \\times 7)$ e deve ser transformada usando a *.view()* na função *forward()* para $(B, 128, 7, 7)$ para entrar em *self.deconv*. A saída da *self.deconv* deve ter dimensões $(B, 1 \\times 28 \\times 28)$, o que são as dimensões de uma sample do MNIST.\n",
        "\n",
        "PS. 2: $D$ deve fazer o caminho inverso de $G$, saindo de *self.conv* com $(B, 128, 7, 7)$, linearizando as dimensões espaciais e canais usando a *.view()* e saindo de $self.fc$ com $(B, 1)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvzdYk6v-gdd",
        "colab_type": "text"
      },
      "source": [
        "# Definindo o Gerador $G$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0FJQQU7-kP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adversarial Generator.\n",
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim=100, output_channels=1, input_size=28):\n",
        "    \n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_channels = output_channels\n",
        "        self.input_size = input_size\n",
        "\n",
        "        d = 128\n",
        "        self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n",
        "        self.deconv1_bn = nn.BatchNorm2d(d*8)\n",
        "        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
        "        self.deconv2_bn = nn.BatchNorm2d(d*4)\n",
        "        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 3, 2, 1)\n",
        "        self.deconv3_bn = nn.BatchNorm2d(d*2)\n",
        "        self.deconv4 = nn.ConvTranspose2d(d*2, 1, 2, 2, 1)\n",
        "        \n",
        "        self.initialize_weights()\n",
        "\n",
        "        \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, z):\n",
        "      z = F.relu(self.deconv1_bn(self.deconv1(z)))\n",
        "      z = F.relu(self.deconv2_bn(self.deconv2(z)))\n",
        "      z = F.relu(self.deconv3_bn(self.deconv3(z)))\n",
        "      z = F.tanh(self.deconv4(z))\n",
        "      return z\n",
        "        \n",
        "# Instantiating G.\n",
        "net_G = Generator(input_dim=args['z_dim']).to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net_G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsrfQ2eOsEZB",
        "colab_type": "text"
      },
      "source": [
        "# Definindo o Discriminador $D$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pai5kiN6DW2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adversarial Discriminator.\n",
        "class Discriminator(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim=1, output_channels=1, input_size=28):\n",
        "        \n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_channels = output_channels\n",
        "        self.input_size = input_size\n",
        "\n",
        "        d = 128\n",
        "        self.conv1 = nn.Conv2d(1, d, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
        "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
        "        self.conv4 = nn.Conv2d(d*4, 1, 4, 2, 1)\n",
        "        \n",
        "        self.initialize_weights()\n",
        "\n",
        "        \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "                \n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = F.leaky_relu(x, 0.2)\n",
        "\n",
        "      x = self.conv2_bn(self.conv2(x))\n",
        "      x = F.leaky_relu(x, 0.2)\n",
        "\n",
        "      x = self.conv3_bn(self.conv3(x))\n",
        "      x = F.leaky_relu(x, 0.2)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = F.sigmoid(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "# Instantiating D.\n",
        "net_D = Discriminator().to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net_D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeUPraJ3EyeP",
        "colab_type": "text"
      },
      "source": [
        "# Definindo o otimizadores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I83m3L4FEzgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO: defining optimizer for G.\n",
        "opt_G = optim.Adam(net_G.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "\n",
        "# TO DO: defining optimizer for D.\n",
        "opt_D = optim.Adam(net_D.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv0tdlhKQYOt",
        "colab_type": "text"
      },
      "source": [
        "# Definindo um Scheduler para os Learning Rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MnD3mr9Qef6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO: defining lr scheduler.\n",
        "scheduler_G = optim.lr_scheduler.StepLR(opt_G, step_size=5, gamma=0.1)\n",
        "scheduler_D = optim.lr_scheduler.StepLR(opt_D, step_size=5, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Cvxt95E-A0",
        "colab_type": "text"
      },
      "source": [
        "# Definindo a loss composta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8SIpVnFCID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO: defining adversarial loss.\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-gUQrX7Fvf-",
        "colab_type": "text"
      },
      "source": [
        "# Criando funções para Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RUHQYKgFxz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training procedure.\n",
        "def train(train_loader,\n",
        "          net_G, net_D,\n",
        "          criterion,\n",
        "          opt_G, opt_D,\n",
        "          epoch,\n",
        "          train_loss_G, train_loss_D):\n",
        "    \n",
        "    tic = time.time()\n",
        "    \n",
        "    # Predefining ones and zeros for batches.\n",
        "    y_real = torch.ones(args['batch_size'], 1).to(args['device'])\n",
        "    y_fake = torch.zeros(args['batch_size'], 1).to(args['device'])\n",
        "\n",
        "    # Setting networks for training mode.\n",
        "    net_D.train()\n",
        "    net_G.train()\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        # Obtaining images and labels for batch.\n",
        "        x, labs = batch_data\n",
        "        \n",
        "        # TO DO: Creating random vector z with size (batch_size, z_dim).\n",
        "        z = torch.randn((args['batch_size'], args['z_dim'])).view(-1, args['z_dim'], 1, 1)\n",
        "        \n",
        "        # TO DO: Casting to correct device (x and z).\n",
        "        x = x.to(args['device'])\n",
        "        z = z.to(args['device'])\n",
        "\n",
        "        y_real_ = torch.ones(args['batch_size']).to(args['device'])\n",
        "        y_fake_ = torch.zeros(args['batch_size']).to(args['device'])\n",
        "        \n",
        "        ###############\n",
        "        # Updating D. #\n",
        "        ###############\n",
        "        \n",
        "        # TO DO: Clearing the gradients of D optimizer.\n",
        "        # ...\n",
        "        opt_D.zero_grad()\n",
        "\n",
        "        # TO DO: Forwarding real data.\n",
        "        D_real = net_D(x) #... # Through D.\n",
        "        \n",
        "        # TO DO: Computing loss for real data.\n",
        "\n",
        "        D_real_loss = criterion(D_real, y_real_) # ...\n",
        "\n",
        "        # TO DO: Forwarding fake data.\n",
        "        G_out = net_G(z) # ... # Through G.\n",
        "\n",
        "        D_fake = net_D(G_out) # ... # Through D.\n",
        "        \n",
        "        # TO DO: Computing loss for fake data.\n",
        "        D_fake_loss = criterion(D_fake, y_fake_) # ...\n",
        "\n",
        "        # TO DO: Computing total loss for D (real + fake).\n",
        "        D_loss = D_real_loss + D_fake_loss # ...\n",
        "        \n",
        "        # TO DO: Computing backpropagation for D.\n",
        "        # ...\n",
        "        D_loss.backward()\n",
        "        \n",
        "        # TO DO: Taking step in D optimizer.\n",
        "        # ...\n",
        "        opt_D.step()\n",
        "\n",
        "        ###############\n",
        "        # Updating G. #\n",
        "        ###############\n",
        "        \n",
        "        # TO DO: Clearing the gradients of G optimizer.\n",
        "        # ...\n",
        "        opt_G.zero_grad()\n",
        "\n",
        "        # TO DO: Forwarding fake data.\n",
        "        G_out = net_G(z) # ... # Through G.\n",
        "        \n",
        "        \n",
        "\n",
        "        D_fake = net_D(G_out) # ... # Through D.\n",
        "        \n",
        "        \n",
        "        # TO DO: Computing loss for G.\n",
        "        G_loss = criterion(D_fake, y_real_) # ...\n",
        "        \n",
        "        # TO DO: Computing backpropagation for G.\n",
        "        # ...\n",
        "        G_loss.backward()\n",
        "        \n",
        "        # TO DO: Taking step in G optimizer.\n",
        "        # ...\n",
        "        opt_G.step()\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss_G.append(G_loss.data.item())\n",
        "        train_loss_D.append(D_loss.data.item())\n",
        "\n",
        "    toc = time.time()\n",
        "    \n",
        "    # Printing training epoch loss.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [training time %.2f]' % (\n",
        "        epoch, (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    \n",
        "    if epoch % args['print_freq'] == 0:\n",
        "        \n",
        "        # Plotting losses.\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
        "\n",
        "        ax[0].plot(np.asarray(train_loss_G), 'r-', label='G loss')\n",
        "        ax[0].legend()\n",
        "        \n",
        "        ax[1].plot(np.asarray(train_loss_D), 'b--', label='D loss')\n",
        "        ax[1].legend()\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "    return train_loss_G, train_loss_D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t2N1kgQNMTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating procedure.\n",
        "def evaluate(net_G, criterion, epoch):\n",
        "    \n",
        "    # Setting networks for training mode.\n",
        "    net_D.eval()\n",
        "    net_G.eval()\n",
        "    \n",
        "    # Creating random vector z.\n",
        "    z = torch.rand((args['num_samples'] * args['num_samples'], args['z_dim']))\n",
        "    \n",
        "    # Casting to correct device.\n",
        "    z = z.to(args['device'])\n",
        "    \n",
        "    # Generating new samples.\n",
        "    G_out = net_G(z)\n",
        "    \n",
        "    # Plotting.\n",
        "    fig, ax = plt.subplots(args['num_samples'],\n",
        "                           args['num_samples'],\n",
        "                           figsize=(8, 8))\n",
        "    \n",
        "    for i in range(args['num_samples']):\n",
        "        \n",
        "        for j in range(args['num_samples']):\n",
        "            \n",
        "            sample = G_out[j * args['num_samples'] + i]\n",
        "            \n",
        "            ax[j, i].imshow(sample.detach().cpu().numpy().squeeze(),\n",
        "                            cmap=plt.get_cmap('gray'))\n",
        "            ax[j, i].set_yticks([])\n",
        "            ax[j, i].set_xticks([])\n",
        "            \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhC6uqUpJO8Y",
        "colab_type": "text"
      },
      "source": [
        "# Iterando sobre epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnbf0HVDJKTC",
        "colab_type": "code",
        "outputId": "c5b66aff-e6f7-42d7-f577-be9fb1b1bd39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Lists for losses.\n",
        "train_loss_G = []\n",
        "train_loss_D = []\n",
        "\n",
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train_loss_G, train_loss_D = train(train_loader,\n",
        "                                       net_G, net_D,\n",
        "                                       criterion,\n",
        "                                       opt_G, opt_D,\n",
        "                                       epoch,\n",
        "                                       train_loss_G, train_loss_D)\n",
        "    \n",
        "    # Taking step on scheduler.\n",
        "    scheduler_G.step()\n",
        "    scheduler_D.step()\n",
        "\n",
        "    if epoch % args['print_freq'] == 0:\n",
        "        \n",
        "        # Testing function for sample generation.\n",
        "        evaluate(net_G, criterion, epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([500])) that is different to the input size (torch.Size([500, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}